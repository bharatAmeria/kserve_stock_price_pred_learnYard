1. Create repo, clone it in local

2. Create a virtual environment -> python3 -m venv venv

3. Activate the virtual environment -> source venv/bin/Activate

4. Create template.py file and copy code and run it ( it create a project structure for you )

5. Add code to setup.py, project.toml, testEnvironment.py, requirements.txt, src/constants/__init__.py.
    Now run testEnvironment.py (verifies the pyhton environment and install dependecies from requirements.txt)

6. add code to src folder, config.yaml.

7. create .env file and add 
    AWS_ACCESS_KEY_ID=
    AWS_SECRET_ACCESS_KEY=
    AWS_S3_BUCKET_NAME=
    AWS_DEFAULT_REGION=
    DATASET_URI=https://drive.google.com/file/d/17nz2lVYeLqbCWDuZb2h2gpROe5BJOe2K/view?usp=sharing

8. Now SignIn in your aws google console create an IAM user and give administrator access.
   Download AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, AWS_DEFAULT_REGION,AWS_S3_BUCKET_NAME.
   Add these credentials to .env file.

9. Now do { aws configure }. It takes AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, AWS_DEFAULT_REGION as input.

10. Now create s3 bucket.  
    aws s3 mb s3://learnyard-data-ingestion-20250726 --region us-east-1

11. Now run train.py file to check each component is working properly.

12. Now create a directory under name app wher we define our fastAPI app. Cpoy code files in the directory.

13. Now create dev folder and copy yaml files.

14. Create folder name kubeflow where our kubeflow pipeline are defined.

15. Build our docker image of our src folder and push to ECR.
    (Do step 17 First)

    (Why ?
      Because our kubeflow pipeline need docerk image to communicate with in order to run pipeline.
      Or 
      we can define each component in the file of kubeflow_pipeline.py which is not best practice.
    )

    aws ecr create-repository --repository-name ml_pipeline

    aws ecr get-login-password --region eu-north-1 | docker login --username AWS --password-stdin 123456789012.dkr.ecr.eu-north-1.amazonaws.com
    docker build --platform linux/amd64 -t ml_pipeline .
    docker tag ml_pipeline:latest 123456789012.dkr.ecr.eu-north-1.amazonaws.com/ml_pipeline:latest
    docker push 123456789012.dkr.ecr.eu-north-1.amazonaws.com/ml_pipeline:latest

    (Replace 123456789012 with your actual AWS account ID.)
    

16. Now copy the ECR image uri and paste it in the kubeflow/kube_flow_pipeline.py file and run kubeflow/ml_pipe.py.
    It create a yaml file dev folder.

17. Now create a cluster to configure kserve and kubeflow
    create eks cluster for kubeflow 
    prerequsite (install eksctl, In this case it is preinstalled) 

    eksctl create cluster \
      --name kubeflow-cluster \
      --version 1.27 \
      --region us-east-1 \
      --nodegroup-name standard-workers \
      --node-type t3.large \
      --nodes 3 \
      --managed

      (take a break it will take around 15 mins or do step 15 and fastAPI step 2)

      

12. Now we need to install Kubeflow

12.1 # Clone the full Kubeflow manifests

      export PIPELINE_VERSION=2.4.0
      kubectl apply -k "github.com/kubeflow/pipelines/manifests/kustomize/cluster-scoped-resources?ref=$PIPELINE_VERSION"
      kubectl wait --for condition=established --timeout=60s crd/applications.app.k8s.io
      kubectl apply -k "github.com/kubeflow/pipelines/manifests/kustomize/env/dev?ref=$PIPELINE_VERSION"

    If you encounter persistent pod crashes

      kubectl apply -k "github.com/kubeflow/pipelines/manifests/kustomize/env/platform-agnostic?ref=$PIPELINE_VERSION"

      kubectl get pods -n kubeflow

      kubectl port-forward -n kubeflow svc/ml-pipeline-ui 8080:80

      minikube image load kfp-local:latest


13. Install kserve:

    Install KServe
    # Install KServe 0.11.2 (recommended stable version)
    kubectl apply -f https://github.com/kserve/kserve/releases/download/v0.11.2/kserve.yaml

    # Wait until all KServe pods are ready
    kubectl get pods -n kserve

      fisrtly install ISTIO :- 

      curl -L https://istio.io/downloadIstio | ISTIO_VERSION=1.20.3 sh -
      cd istio-1.20.3
      export PATH=$PWD/bin:$PATH

      # Knative Serving CRDs
      kubectl apply -f https://github.com/knative/serving/releases/download/knative-v1.10.1/serving-crds.yaml

      # Knative Serving Core
      kubectl apply -f https://github.com/knative/serving/releases/download/knative-v1.10.1/serving-core.yaml

      # Knative Istio integration (both controller + config)
      kubectl apply -f https://github.com/knative/net-istio/releases/download/knative-v1.10.1/net-istio.yaml

14. Install Cert-Manager (dependency for KServe)
      kubectl apply -f https://github.com/jetstack/cert-manager/releases/download/v1.13.2/cert-manager.yaml

      # Wait until all cert-manager pods are ready
      kubectl get pods -n cert-manager


      Wait until all pods are running in the knative-serving namespace:

      kubectl get pods -n knative-serving
      kubectl get svc -n istio-system

      kubectl apply -f inference-service.yaml

      kubectl get svc istio-ingressgateway -n istio-system

Look for the EXTERNAL-IP. Example: a1b2c3d4e5.us-east-1.elb.amazonaws.com
      http://a1b2c3d4.elb.amazonaws.com
----------- Fast API ---------------

1. Dockerize FastAPI App
2. Build and push image to ECR:

    aws ecr create-repository --repository-name fastapi-inference

    # Authenticate & push
    aws ecr get-login-password | docker login --username AWS --password-stdin <your-ecr-url>
    docker buildx build --platform linux/amd64 -t 12345678910.dkr.ecr.us-east-1.amazonaws.com/lambda:latest .
    docker push 12345678910.dkr.ecr.us-east-1.amazonaws.com/lambda:latest

5. 

6. Verify:
    kubectl get deployment -n kserve
    kubectl get svc -n istio-system

7. 
   kubectl apply -f InferenceService.yaml
   kubectl get inferenceservices







3. Install KServe if not already:
    kubectl apply -f https://github.com/kserve/kserve/releases/download/v0.11.2/kserve.yaml

4. kubectl apply -f InferenceService.yaml

5. kubectl get inferenceservices
   kubectl get pods

6. Access the FastAPI Endpoint
   kubectl port-forward svc/istio-ingressgateway -n istio-system 8080:80

7. Hit the url

http://localhost/v1/models/fastapi-serve:predict



---------------- delete resources used ------------------

eksctl delete cluster --name kubeflow-cluster --region us-east-1

# Delete all Load Balancers created by EKS
aws elb describe-load-balancers --region us-east-1 | jq -r '.LoadBalancerDescriptions[].LoadBalancerName' | while read lb; do
  echo "Deleting ELB: $lb"
  aws elb delete-load-balancer --load-balancer-name $lb --region us-east-1
done

aws elbv2 describe-load-balancers --region us-east-1 | jq -r '.LoadBalancers[].LoadBalancerArn' | while read lb; do
  echo "Deleting ELBv2: $lb"
  aws elbv2 delete-load-balancer --load-balancer-arn $lb --region us-east-1
done

aws ec2 describe-volumes --region us-east-1 --query "Volumes[*].[VolumeId,State]" --output text | grep available | awk '{print $1}' | while read vol; do
  echo "Deleting EBS Volume: $vol"
  aws ec2 delete-volume --volume-id $vol --region us-east-1
done

aws ec2 describe-security-groups --region us-east-1 --query 'SecurityGroups[*].GroupId' --output text | tr '\t' '\n' | while read sg; do
  echo "Trying to delete SG: $sg"
  aws ec2 delete-security-group --group-id $sg --region us-east-1 2>/dev/null
done

aws ec2 describe-vpcs --region us-east-1 --query 'Vpcs[*].VpcId' --output text
# Then delete:
aws ec2 delete-vpc --vpc-id <vpc-id> --region us-east-1

aws s3 ls
aws s3 rb s3://<bucket-name> --force
aws ecr delete-repository --repository-name lambda --region us-east-1 --force
